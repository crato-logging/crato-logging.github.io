<!DOCTYPE html>

<html lang="en-US" prefix="og: http://opg.me/ns#">
  <head>
    <meta charset="UTF-8" />

    <meta name="title" property="og:title" content="Crato" />

    <meta
      name="description"
      property="og:description"
      content="Crato is an open source framework for small web applications to easily deploy a centralized logging solution that maintains ownership of data"
    />

    <meta name="type" property="og:type" content="website" />

    <meta
      name="url"
      property="og:url"
      content="https://crato-logging.github.io/"
    />

    <meta
      name="image"
      property="og:image"
      content="images/logos/crato-logo.png"
    />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta
      name="author"
      content="Faazil Shaikh, Kurth O'Connor, Alex Soloviev"
    />

    <title>CRATO</title>

    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/icons/favicon_package_v0.16/favicon-16x16.png"
    />

    <link
      href="https://fonts.googleapis.com/css?family=Hind|Hind:700|Open+Sans:700|Teko:700&display=swap"
      rel="stylesheet"
    />

    <!-- <style>reset</style> -->

    <link rel="stylesheet" href="stylesheets/reset.css" />

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/gruvbox-dark.min.css"
      charset="utf-8"
    />

    <!-- <style></style> -->

    <link rel="stylesheet" href="stylesheets/main.css" />

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

    <!-- <script></script> -->

    <script src="javascripts/application.js"></script>
  </head>

  <body>
    <div class="logo-links">
      <p id="crato-logo">CRATO</p>

      <a href="https://github.com/crato-logging/crato" target="_blank">
        <img
          src="images/logos/github_black.png"
          alt="github logo"
          id="github-logo"
        />
      </a>
    </div>

    <nav id="site-navigation">
      <ul>
        <li>
          <a href="#home" id="home-link">HOME</a>
        </li>

        <li>
          <a href="#case-study" id="case-study-link">CASE STUDY</a>

          <nav id="case-study-mobile">
            <ul></ul>
          </nav>
        </li>

        <li>
          <a href="#our-team" id="our-team-link">OUR TEAM</a>
        </li>
      </ul>
    </nav>

    <header id="home">
      <h1>
        <img src="images/logos/crato-logo.png" alt="Crato logo" />

        <p>easy log management for small applications</p>
      </h1>
    </header>

    <section class="integration">
      <div class="box">
        <img src="images/crato_horizontal.png" alt="best practices" />
      </div>

      <article class="box">
        <div class="text-box">
          <h1>Centralize your logs</h1>

          <p>
            Crato is an open-source framework for small applications to easily
            deploy a centralized logging solution that maintains ownership of
            data.
          </p>

          <!-- <a class="button" href="#case-study">Learn More</a> -->
        </div>
      </article>
    </section>

    <main>
      <section id="case-study">
        <h1>Case Study</h1>

        <div id="side-nav">
          <img src="images/logos/crato-logo.png" alt="Crato logo" />
        </div>

        <nav>
          <ul></ul>
        </nav>

        <h2 id="introduction">1 Introduction</h2>

        <h3>1.1 What is Crato?</h3>

        <p>
          Crato is an open source framework for small applications to easily
          deploy centralized logging.
        </p>

        <p>
          Applications and system services record their events to local log files,
          but by default these log messages remain on local systems and are eventually 
          either archived away never to be seen or permanently deleted. Without 
          intervening then, developers and administrators lose valuable insight 
          into the history, performance and security of their systems and applications.
        </p>

        <p>
          Crato allows developers to quickly and easily deploy a centralized 
          logging infrastructure to address this problem. Using Crato, each 
          machine sends its logs to a central collection server, where
          developers can gain insight into their entire system by analyzing the
          consolidated logs across the entire deployed fleet of systems and applications. 
          Besides log data consolidation and analysis (via InfluxDB), Crato also 
          provides smart compression and archiving (via Amazon S3 and S3 Glacier). 
          Crato makes configuring, deploying, and maintaining all these pieces easy and fast.
        </p>

        <p>
          Before we get into Crato, let's take a step back and discuss what logs are. 
        </p>

        <h2 id="logging">2 Logging</h2>

        <h3>2.1 What is a Log?</h3>

        <p>
          Developers rely on logs of all sorts on a daily basis. In
          the simplest case, we can think of logs as distinct events that get
          written to files<sup><a href="#footnote-1">1</a></sup>. Log data is 
          often the primary way to determine application health, debug a 
          problem, or derive aggregate metrics. It's the first place any 
          developer looks to try to piece together what happened in an event of 
          an error or exception. Logs are critical and every piece of software, 
          from infrastructure to operating system services, are continuously 
          logging events as soon as the system is up.
        </p>

        <h3>2.2 Logs as Files</h3>

        <p>
          In most UNIX systems, these events are written to log files. For the 
          most common applications and services, these files can be found in the 
          <code>var/log</code> directory. Within that directory, some services 
          will create sub-directories to organize their logs. For example, MySQL 
          generates its own log file and the <code>httpd</code> sub directory 
          contains log files for the Apache web server <code>access_log</code> 
          and <code>error_log</code>. There are also system process logs that 
          can be found in files like <code>kern.log</code> or <code>auth.log</code> 
          that contain information logged by the kernel and system authorization 
          information, respectively.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/logs_as_files.png" alt="Logs as files" />
        </div>

        <p>
          Because log messages are generated by some process and get written to
          files on our local file system, it's natural to think of logs simply
          as files. If logs are just files, then we use commands like
          <code>grep</code> to match against some string or regular expression
          in the log file. 
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/grep.gif"
            class="softened"
            id="grep"
            alt="Logs as files"
          />

          <p>
            We can use <code>grep</code> to search a file for logs generated in the last 
            10 minutes.
          </p>
        </div>

        <p>
          But in order to better understand the structure of application logs, 
          let's consider another description of what a log is. We know that a log 
          is a file but that's not the full story.
        </p>

        <h3>2.3 Logs as Append-Only Data Structures</h3>

        <blockquote>
          <p>
            "A log is perhaps the simplest possible storage abstraction. It is
            an append-only, totally-ordered sequence of records ordered by time"
          </p>

          <p>- Jay Kreps</p>
        </blockquote>

        <p>
          In the simplest case, we have an application that runs on a single
          server, and our life is relatively easy. When the application is 
          contained on one server, so is its log data - when something goes
          wrong we know where to look and it's fairly straightforward to paint 
          the entire picture of the application state. However, as applications 
          continue to grow in complexity so does the volume of their log data - 
          the many components, devices and microservices that produce log data 
          are often distributed on multiple servers and even with a relatively 
          small number of users, this can quickly become a big problem of 
          incomplete information. For this reason there has been a greater need 
          to unify log data, both by location as well as format.  On top of data 
          consolidation, there's also been a growing need to synthesize the 
          proliferation of logs for business insights. This also implies a growing 
          need for machines to process log data.
        </p>

        <p>
          In order for a machine to parse and aggregate log data to
          then be consumed by humans, a text file may no longer be sufficient.
          But how do we store log data, if not in a file?
        </p>

        <p>
          The key is to recognize that logs as files are just one form of a more
          general concept of a log as a data structure. That is, if we regard a log as
           a time-ordered, append only sequence of records, we can see log files 
           as just one example. In this light, a log and its basic properties can
           be represented by the following diagram.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/log_data_structure.png" alt="Log" />
        </div>

        <p>
          Each entry in a log is added sequentially making this an append-only
          data structure. We start on the left and each subsequent entry is then
          added to the right. The entries on the left are then by definition
          older than entries on the right, giving this data structure a built-in 
          ordering<sup ><a href="#footnote-2">2</a></sup >. Since reads also 
          proceed from left to write, in the context of building an application, 
          if each entry in a log represents some event, we can know the order 
          in which the events occurred. In this way, logs provide a historical 
          context for our applications.
        </p>

        <h3>2.4 Logs as Streams</h3>

        <p>
          The "Twelve-factor App" is a set of best practices for building robust 
          and maintainable web applications. One of the guidelines directly address 
          logging, suggesting that we should treat logs not as files but
          as event streams, and that a web application shouldn’t concern itself
          with the storage of this information. Instead, these logs should be
          treated as a continuous stream that is captured and stored by a
          separate service<sup><a href="#footnote-3">3</a></sup >.
        </p>

        <p>
          Of course it's natural to think of logs as files since it's a common 
          way to interact with logs and because log file generation is typically 
          an automatic process that happens behind the scenes. But log messages 
          get written to files mainly because it's a convenient method of storing 
          data. As we have seen, outside of the context of a file, individual log 
          messages can be viewed as time ordered streams of events that <em>can</em> 
          be written to files but that can also be processed in other ways. To 
          borrow another quote from Jay Kreps, "Log is another word for stream 
          and logs are at the heart of stream processing".  Streams of data can 
          be directed or split into multiple streams and sent to different locations 
          for further processing.
        </p>

        <p>
          If we use a command line utility like <code>tail</code> it's possible
          to see a live stream of logs that are coming from an application or
          any other process we'd like to monitor. So while messages are being
          written to files, from this view it's easier to think of logs as a
          collection of events that we can view in real time as they occur 
          - something that's ongoing, with no beginning or an end.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/tail.gif"
            class="softened"
            id="tail"
            alt="Logs as streams"
          />

          <p>
            Ongoing, collated collection of events viewed in real time with the
            <code>tail</code> utility.
          </p>
        </div>

        <h3>2.5 Logs Capture Change</h3>

        <p>
          To review, a log at its core is an append- only sequence of records 
          that has a natural notion of "time" because newer records are appended 
          to the end of the log. Taken together, these properties mean that a 
          log effectively captures changes over time, thereby <i>creating a full 
            history of the application</i>.
        </p>

        <p>
          We most often think of our application's database, where we keep the
          busines logic of our application, as the source of truth. If some data 
          on the UI doesn't look right, we consult the database to determine 
          whether it's correct. But even a database can't tell you how the data 
          got to that state. In some situations, knowing the historical context 
          for how we got to a particular state can be even more important.  As 
          espoused by Martin Kleppman, Jay Kreps, and others, this idea is known 
          as the "Stream-Table Duality".
        </p>

        <h3>2.6 Stream-Table Duality</h3>

        <p>
          Stream-Table duality is another way of saying that there are distinct
          but related ways to represent data. The familiar way is of course with
          a table which represents a snapshot of current state. Let's consider 
          a simple example below using relational data in tables. There are two users
          and they are transferring funds between one another. As changes occur
          over time, the data in a table is updated to reflect a new current
          state.
        </p>

        <div class="buttons">
          <button id="resetButton" class="cratoButton">Reset</button>

          <button id="nextButton" class="cratoButton">&rarr;</button>
        </div>

        <div class="img-wrapper">
          <img
            src="images/diagrams/table_states/state1.png"
            id="table-state"
            data-id="1"
            alt="Table State1"
          />

          <p>
            As changes occur over time, the data in a table is updated to
            reflect a new current state. But how did we get there?
          </p>
        </div>

        <p>
          From this example, we can see that data in the table is mutable since the
          old values are replaced by new values. When we isolate a table and 
          look at it at any point in time, it's possible to answer questions 
          about the current state - for example, what <em>is</em> the current 
          balance of a customer with username big_ed.  However, looking only at a 
          database record at one point in time doesn't paint a full picture - 
          there are many ways you could have arrived from one state to another 
          and it is only the log that can describe that journey. The log is the 
          database's changelog and it's there that we can find the full story of 
          how the current data arrived at its current state.
        </p>

        <p>
          Representing the same history that we saw above but this time as a
          stream of changes would look like this:
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/gifs/stream.gif" alt="Stream" />

          <p>
            A stream captures the changes between the table's states as distinct 
            events.
          </p>
        </div>

        <p>
          From a log-as-stream perspective, the data is immutable since the 
          values are not updated. Instead, distinct and immutable events are 
          appended to the end of the log. What is captured in the blue boxes 
          are the changes between the table's states. This stream of changes can 
          supplement the core business logic that lives in tables and helps 
          answer not only <em>what</em> changed but also <em>why</em> and 
          <em>how</em> it changed<sup ><a href="#footnote-4">4</a></sup >.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/stream_table_duality.png"
            alt="Stream-table duality"
          />

          <p>
            Tables show us current state and streams capture how we got there.
          </p>
        </div>

        <p>
          And a log is useful in another way besides providing historical context 
          for existing data. Because a log captures the changes themselves, it is 
          possible to rebuild the current state of any database from the database 
          logs alone. In fact, it is possible to rebuild the "current state" of 
          any point in time of the database from logs.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/stream_to_table.png"
            alt="Stream to a table"
          />

          <p></p>
        </div>

        <p>
          In this way, it's possible to think of a log as the real source of 
          truth since we can recreate a snapshot of the database at any point 
          in time from logs. That is, we can aggregate streams to arrive at
          tables, but we cannot understand the historical context from only tables.
        </p>

        <h3>2.7 Why Does Logging Matter for Small Applications?</h3>

        <p>
          There are obvious benefits to logging that are important in day to day
          development. Developers use logs every day for analysis and debugging,
          infrastructure monitoring and for deriving useful metrics. Some types
          of applications are also required to keep logs long term for audit
          purposes. However, besides these uses, careully curated logs related
          to your application can provide valuable historical context that
          unlocks other benefits:
        </p>

        <ul>
          <li>We can see precisely see how our applications are being used</li>
          <li>Data can be used to find trends, patterns, and behaviors</li>
          <li>With historical event logs we can rebuild a snapshot of our 
            application state at any point in time</li>
        </ul>

        <p>
          For a small but growing application, there are many options to get
          started with logging. Despite the myriad of options, many companies 
          opt to deprioritize logging from the outset. Yet, the power of logs 
          can only be realized later; by the time an application owner decides 
          she wants to look at historical data, it's too late to get started as 
          previous data is already lost. By default, many systems delete log data; 
          in some cases, it's archived into various machines and unusable for 
          meaningful analysis. Most small application owners do not think about a 
          logging strategy until significant data is lost. 
        </p>

        <p>
          In the next section, we'll discuss how to get started with 
          syslog, the standard log recording format on UNIX, and 
          the various architectural deployment options that small application 
          owners have for centralizing and storing their log data.
        </p>

        <h2 id="syslog">3 Syslog</h2>

        <h3>3.1 Single Host Architecture</h3>

        <p>
          Previously, we talked about how log files are automatically generated 
          by a logging system on the operating system.  Syslog is the most widely 
          used logging system. In fact, syslog is a UNIX standard that defines 
          a message format and allows for stanardization of how logs are 
          generated and processed. 
        </p>

        <p>
          Since the original inception of Syslog in the 1980's, there are now
          various implementations of the syslog daemon, with rsyslog and
          syslog-ng being the most notable examples. For the purposes of our
          discussion here, we'll use the generic syslog daemon in our diagrams,
          however any of the more modern implementations can be used in its
          place.
        </p>

        <div class="img-wrapper">
          <img src="images/diagrams/syslog_local.png" alt="Syslog local" />
          <p></p>
        </div>

        <p>
          First let's take a look at what syslog looks like locally. In a simple
          case, for example on a single node system, the log daemon will receive
          log messages and then write them to the local file system as we saw
          earlier - typically the files are written to the
          <code>var/log</code> directory.
        </p>

        <p>
          There is a separate process called "log rotation" that is responsible
          for compressing, archiving and eventually removing old log files. Log
          rotation occurs daily, weekly or when a file reaches a certain size -
          this depends on the configuration<sup ><a href="#footnote-5">5</a></sup >. 
          For example, you can choose to keep 3 days worth of logs, in which
          case the log rotation process will rename the log file to make room
          for the next file, eventually leaving the most recent three files:
          <code>example.log.1</code>, <code>example.log.2</code> and
          <code>example.log.3</code>
        </p>

        <p>
          During the next iteration, the <code>example.log.3</code> file will
          get rotated out and <code>example.log.2</code> file takes its place
          and is renamed <code>example.log.3</code>.
        </p>
        <div class="img-wrapper">
          <img
            src="images/diagrams/syslog_local2.png"
            alt="Syslog local on client"
          />
          <p></p>
        </div>

        <p>
          This process occurs behind the scenes in a localized system. But 
          applications are rarely deployed on a single node. There
          are typically many moving parts and various components across
          different servers. Each of these components is generating its own log
          data in real time, so the question is how do you as a developer sift
          through this data and correlate events from each component?
        </p>

        <p>
          Imagine that you have an error and you need to refer to your logs to
          see what went wrong. In this case you would have to individually
          connect to every single one of the servers one at a time, locate logs
          on each machine and try to find the information that you are looking
          for, and then try to correlate different events across different machines, 
          perhaps by their timestamp, to understand what went wrong.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/local_logging.gif"
            alt="Localized logging"
          />
          <p></p>
        </div>

        <p>
          This is not what we want. Having all these different logs hosted in
          different locations makes the correlation and analysis difficult and
          on top of that, through the process of log rotation, it might be 
          possible that the log file you're looking for has been rotated out and 
          deleted forever. Ideally we would bring together all these logs into 
          one central place, where we can analyze the logs in aggregate and 
          come up with a uniform back-up strategy for them. 
        </p>

        <h3>3.2 Forwarding Messages to a Central Server</h3>

        <p>
          Syslog can also help with forwarding log events across machines. It 
          does this by having facilities for both sending and receiving log events: 
          a syslog daemon for each log-sending machine and a syslog daemon on the 
          log-receiving machine where all log data is to be centralized.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/syslog_centralized.png"
            alt="Centralized syslog"
          />
          <p></p>
        </div>

        <p>
          This time, the syslog daemon on each of your components captures log
          messages but instead of simply writing them to a file, it forwards messages
          to a remote machine where another syslog daemon accepts them. Here, 
          it's possible to view our logs as a consolidated whole. Already this 
          is a big improvement from the disjointed scenario we had above.
        </p>

        <p>
          Now you can inspect all your logs from a single place and you don’t
          have to rely on connecting to every machine individually, you can
          directly see the logs from the same filesystem. This also makes for a
          far more reliable way to access logs because even if one of these
          clients becomes inaccesible, all logs up to the point of the client's
          failure are already in the central server.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/centralized_logging.png"
            alt="Centralized logging"
          />
          <p></p>
        </div>

        <p>
          Now we have a much more desirable and more manageable situation and 
          this is the basis for a centralized logging architecture. Once all logs 
          that are relevant to your application are consolidated in one place, 
          their value in aggregate starts to become more apparent.
        </p>

        <p>
          Whether you want to use simple command line utilities like <code>grep</code>, 
          ship your logs somewhere else, or visualize them using the many monitoring 
          and visualization tools that exist, it can only be possible once the 
          logs are centralized. In the next section, we'll enumerate over some 
          solutions to help developers set up a centralized logging architecture.
        </p>

        <h2 id="current-solutions">4 Current Solutions</h2>

        <p>
          What solutions exist for a small and growing application that wants to 
          manage its own logs?
        </p>

        <h3>4.1 Three Main Options</h3>

        <div class="img-wrapper">
          <img
            src="images/three_options.png"
            class="softened"
            alt="options for centralized logging"
          />

          <p>
            Three main types of log management solutions
          </p>
        </div>

        <p>
          To quickly survey the available options, let's distinguish between 
          three main categories of log management solutions. <em>Logging 
          as a Service</em> (LaaS) consists of paid, propietary, typically 
          cloud-based systems.  The <em>Do it Yourself</em> (DIY) path 
          involves designing and building a custom, self-hosted solution from a 
          variety of components. Finally, the <em>Open-Source Software</em> (OSS) 
          solutions are open-source, pre-designed centralized logging systems that
          you deploy on your own machines.
        </p>

        <h3>
          4.2 LaaS: Full Service Log Management
        </h3>

        <p>
          Full-service Logging as a Service (LaaS) products provide their customers 
          with the full range of options, consisting of the following five log 
          management functions:
        </p>

        <ol>
          <li>Log Collection</li>
          <li>Ingestion &amp; Storage</li>
          <li>Search &amp; Analysis</li>
          <li>Monitoring &amp; Alerting</li>
          <li>Visualization &amp; Reporting</li>
        </ol>

        <p>
          LaaS solutions are generally cloud-based. They&#39;re designed to be 
          easy-to-use, and they provide a rich feature-set along with technical 
          support. The most difficult part in working with a LaaS may be configuring 
          your machine(s) to forward logs into the service, and a LaaS will typically
          provide tools and instructions for that as well. The LaaS handles the 
          rest. The convenience of a LaaS is hard to deny, and products in this
          category provide ideal solutions for many application's needs.
        </p>

        <p>
          The drawbacks however are also apparent. The services aren't free, and 
          costs can fluctuate based on a number of factors, including the volume of
          data being ingested, the number of users, and storage or retention plans. 
          Nor is it simply an issue of costs. Whether cloud-based or on-premise, 
          propietary solutions are marked by a lack of ownership: users cede control
          of their data to a third party, and they are unable to inspect, modify, or 
          audit proprietary code.
        </p>

        <p>
          Evaluating these tradeoffs, some developers opt to look elsewhere for 
          log management, perhaps in search of greater data control, unique features, 
          and / or reduced costs.  If a LaaS is not your preferred solution, another
          option would be to build your own.
        </p>

        <h3 id="diy">4.3 DIY</h3>

        <p>
          If you decide to Do It Yourself (DIY), the most common approach is to
          design and build a system out of pre-existing components. You would
          select these components based on your application's needs and the components'
          interoperability. The basic categories of components are below.
        </p>

        <ol>
          <li>Log Collection</li>
          <li>Processing &amp; Aggregration</li>
          <li>Ingestion &amp; Storage</li>
          <li>Monitoring &amp; Visualization</li>
        </ol>

        <div class="img-wrapper">
          <img
            src="images/diagrams/components/components.png"
            class="softened"
            alt="key components in log management"
          />

          <p>
            Key components in a log management system
          </p>
        </div>

        <h4 id="collection">4.3.1 Collection</h4>
        <p>
          Log collection occurs on each machine with an agent listening for 
          messages generated by applications and system processes. When logs are 
          generated in response to new events, the agent collects them.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/components/zoomed_in/log_collection.png"
            class="softened"
            alt="log collection"
          />

          <p>
            Logs are collected from each client machine
          </p>
        </div>

        <h5 id="syslog-protocol-its-implementations">
          4.3.1.1 Syslog Protocol &amp; its Implementations
        </h5>

        <p>
          Syslog, and its more recent implementations rsyslog and syslog-ng, are
          commonly found on UNIX systems, and are even available on Windows. 
          Because of this ubiquity, DIY systems are often able to directly work
          with one or another of these syslog implementations for log collection. 
          But there are other options as well.
        </p>

        <h4 id="processing-aggregration">4.3.2 Processing &amp; Aggregation</h4>

        <div class="img-wrapper">
          <img
            src="images/diagrams/components/collection_aggregation.png"
            class="softened"
            alt="log collection and aggregation"
          />

          <p>
          Once collected from each client, logs are aggregated in a central location
          </p>
        </div>

        <p>
          After collecting logs from each client, the next step is to aggregate 
          these logs on a single machine. To accomplish this, a logging system 
          must 'ship' or forward the logs to a centralized server. Here, the 
          logs may be parsed and transformed into formats like JSON, enriched 
          with context and stored.  There is a healthy amount of options from 
          which to choose here, and choices will depend largely on application need.
        </p>


        <h4 id="ingestion-storage">4.3.3 Ingestion &amp; Storage</h4>
        <p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/components/zoomed_in/ingestion_storage2.png"
            class="softened"
            alt="log ingestion"
          />

          <p>
            Logs ingested into storage
          </p>
        </div>

        <p>
          Once logs are collected, the goal is to transport them to a storage
          service from which they can be searched and analyzed. If the write rate 
          of the chosen datastore is compatible with potentially bursty influx 
          of logs into the system, then the logs could be sent directly to storage. 
          However, not all applications will be able to rely on this throughput parity.
          Applications that need to scale, or that expect wide fluctuations in data 
          flow, would likely do better by decoupling log aggregation and storage. 
          This can be handled in a number of ways, and a common option would be
          to use a messaging system. Doing so would provide a more robust, 
          fault-tolerant system, along with the ability to handle higher throughput. 
          But it also introduces additional complexity.
        </p>

        <p>
          Here, as with the other components, there are a number of options to 
          choose from. The design and implementation choices will depend on a 
          number of factors. Relevant considerations include the following: 
        </p>

        <ul>
          <li>How much data is being ingested?</li>
          <li>Who is going to be using the data?</li>
          <li>What is the structure of the log?</li>
          <li>What search, analysis and indexing capabilities are needed?</li>
        </ul>

        <h4 id="monitoring-visualization">
          4.3.4 Monitoring &amp; Visualization
        </h4>
        <p>
          Finally, once data is in a database, we can connect it to a
          visualization tool for more user friendly analysis and reporting. Some
          of these tools also provide alerting and system configuration
          capabilities. The visual representation of data allows users to
          transform and aggregrate in more effective ways. There are many open
          source options at this space as well with Kibana and Grafana as the
          leaders.
        </p>

        <p>
          If you decide to DIY, at each phase of a central log management
          solution there are several options available, but also many
          considerations to be made. Researching and orchestrating them to work
          and scale is also a challenge.
        </p>

        <p>
          Overall, doing it yourself (DIY) and building a custom system allows
          for greater control and flexibility at the cost of complexity. Custom
          systems address exact needs allowing for high control of storage and
          security.
        </p>

        <h3 id="oss">4.4 OSS</h3>
        <p>
          Open source log management tools are available that allow a developer
          to maintain high data control, ease-of-use closer to LaaS tools, but
          require self-management and hosting. The two main solutions here are
          the Elastic stack and Graylog.
        </p>

        <h4 id="elastic-stack">4.4.1 Elastic Stack</h4>
        <p>
          Elastic is a company that develops a collection of open-source log
          management products designed to work well together. Altogether, they
          have products that cover each of the 4 key phases of centralized log
          management. A very common combination of their tools: The text-search
          storage system Elasticsearch, log collection and storage service
          Logstash and visualization/monitoring tool Kibana is known as the ELK
          stack.
        </p>
        <h4 id="graylog">4.4.2 Graylog</h4>
        <p>
          Graylog is a company and open-source centralized log management
          server. It is built on top of Elasticsearch and MongoDB and offers a
          web interface connected to the server.
        </p>

        <p>
          The Elastic Stack and Graylog are very popular open-souce self-hosted
          solutions and offer expected usability, high data ownership and
          security. A rich ecosystem of plugins exists for each tool offering
          extended flexibility. However, complexity from these systems comes
          from configuration, deploying scaling up and orchestration related to
          these challenges.
        </p>

        <h3 id="summary-current-log-management-options">
          4.5 Summary: Log Management Options
        </h3>
        <p>
          LaaS tools are easy to use, but offer low data ownership and
          potentially high monetary costs.
        </p>
        <p>
          DIY offers high data ownership, but some costs in developer time and
          higher complexity in building a custom system.
        </p>
        <p>
          OSS tools are free and offer high data ownership, but scaling and
          maintenance are still somewhat complex.
        </p>

        <h2 id="crato-design">5 Crato Design</h2>

        <p>
          The above discussion of current log management options reveals the
          niche that Crato addresses. While LaaS options provide the quickest
          and most simple path, their advantages come with costs. Not only must
          users pay for LaaS services, but their applications' data is yielded
          to a third party. DIY options grant users the most control, and can be
          designed and built to include precisely the features wanted. But there
          are tradeoffs here as well. The high level of control and
          customizability comes with increased complexity and additional design
          and maintenance committments. Finally, the pre-designed OSS options
          aim to split the difference between LaaS and DIY. OSS options return
          data control to the users (typically lacking in LaaS) while reducing
          complexity and design and maintenance commitments (both of which tend
          to be high in DIY projects).
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/complexity_ownership_diagram.png"
            alt="2-axis-comparison"
          />
        </div>

        <p>
          But even here in the OSS options, there is a range of complexity,
          which is largely attributable to the database and full-text search
          choices of the OSS stacks. Crato reduces complexity even more by
          eliminating the need to set up and maintain a full-text search engine.
          Thus, Crato is designed to provide the core features of centralized
          logging while reducing complexity and time commitments.
        </p>

        <h4>Design Goals</h4>

        <ul>
          <li>Open Source</li>
          <li>Provide core centralized logging functions</li>
          <li>
            Easy to use: knowledge and skills for configuration, deployment and
            use are likely to be found in the development team
          </li>
          <li>
            Lightweight: modify as little as possible on client machines; do not
            require changes to user's application code
          </li>
          <li>Allow data ownership and control over storage</li>
        </ul>

        <h3 id="51cratoarchitectureoverview">
          5.1 Crato Architecture Overview
        </h3>

        <p>
          Crato consists of two main components: the <i>Crato Client</i> and
          <i>Crato Core</i>. The client is installed and run on each machine
          from which logs are to be collected. The core infrastructure is
          deployed via Docker onto a separate server or VPS.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_architecture.png"
            alt="crato architecture"
          />
        </div>

        <h3 id="52commands">
          5.2 Commands
        </h3>

        <h4 id="521cratoclient">
          5.2.1 Crato Client
        </h4>

        <p>
          Crato Client is designed to have a light footprint on each client
          system. Downloading the client adds only two files to each system: the
          <code>crato</code> bash script and <code>crato-config</code>, a text
          file to record user preferences. Running Crato client interferes with
          the native logging of the system as little as possible. Crato adds
          only a single configuration file that supplements, not overrides,
          default settings.
        </p>

        <p>
          To download Crato client, <code>curl</code> or <code>wget</code> the
          script from {url}.
        </p>

        <h4 id="cratoconfig"><code>crato-config</code></h4>

        <div class="img-wrapper">
          <img
            src="images/crato_config_file.png"
            class="softened"
            alt="crato client config file"
          />
        </div>

        <p>
          To simplify deployment and reduce complexity, the Crato client cli
          exposes only a handful of configuration options. Crato client
          configuration options are set in the <code>crato-config</code> text
          file. Using key-value pairs, enter any number of logs for Crato to
          track, along with their tags and severity levels. Then enter the ip
          address of the central server.
        </p>

        <p>
          For users needing more granular control over settings, the full range
          of configuraton options are available by directly editing the default
          <code>*.conf</code> files. Because Crato does not override
          applications' logging processes, these can be configured as before.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/gifs/crato_usage.gif"
            class="softened"
            alt="client cli usage"
          />
        </div>

        <h4 id="cratoc">
          <code>crato -c</code>
        </h4>

        <p>
          Apply the current configuration and start Crato client with
          <code>crato -c</code> or <code>crato --configure</code>. Running this
          command checks that permissions, <code>rsyslog</code> version and base
          configuration is appropriate for Crato client. It then dynamically
          generates a <code>/etc/rsyslog.d/49-crato.conf</code> file to hold
          Crato configuration, and checks the validity of the resulting
          <code>rsyslog</code> configuration. It then either throws an error
          with instructions for debugging or (re)starts logging with the new
          settings.
        </p>

        <h4 id="cratosandcrator">
          <code>crato -s</code> and <code>crato -r</code>
        </h4>

        <p>
          <code>crato -s</code> or <code>crato --suspend</code> suspends Crato's
          logging on the client and returns the machine to default logging.
          <code>crato -r</code> or <code>crato --resume</code> restarts Crato
          logging on the client with the suspended settings resumed. If Crato
          client was not suspended at the time of execution, then the command
          makes no changes.
        </p>

        <h4 id="cratod">
          <code>crato -d</code>
        </h4>

        <p>
          <code>crato -d</code> or <code>crato --delete</code> removes current
          or suspended Crato client configurations, confirms that resulting
          default configuration is valid and restarts logging with default
          settings or throws an error with instructions for debugging.
        </p>

        <h4 id="cratoh">
          <code>crato -h</code>
        </h4>

        <p>
          <code>crato -h</code>, <code>crato --help</code>, or even
          <code>crato</code>
          without flags provides help by displaying a usage screen.
        </p>

        <h4 id="522cratocore">
          5.2.2 Crato Core
        </h4>

        <div class="img-wrapper">
          <img src="images/diagrams/gifs/crato_docker_deploy.gif" 
          alt="deployment via Docker" />

          <p>Crato infrastructure installs and deploys with just two commands</p>
        </div>

        <h2 id="crato-implementation">6 Crato Implementation</h2>

        <h3 id="61collectionaggregation-1">
          6.1 Collection &amp; Aggregation
        </h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_collection_aggregation.png"
            alt="collection and aggregation"
          />
        </div>

        <p>
          Following its design mandate of being a lightweight, easy-to-use
          system, Crato collects logs using <code>rsyslog</code>, a standard
          tool available on most <code>*nix</code> systems.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/client_system.png"
            alt="inner client diagram"
          />

          <p>
            System logs and designated application logs collected into single
            stream.
          </p>
        </div>

        <p>
          At the user’s discretion, Crato can collect any text-based log
          messages your servers, applications, databases, etc generate. Captured
          log messages are consolidated into a single stream along with system
          logs. Together, these messages are persisted to disk in the
          <code>syslog</code> file on each client machine and also routed for
          processing and forwarding.
        </p>

        <h3 id="62shippingingestion">
          6.2 Shipping &amp; Ingestion
        </h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_shipping_ingestion.png"
            alt="shipping and ingestion"
          />

          <p>
            Crato clients monitor logs, forwarding messages as they arrive for
            ingestion into the core Crato infrastructure.
          </p>
        </div>

        <h4 id="621shipping">
          6.2.1 Shipping
        </h4>

        <p>
          So far, the logs are grouped into a single stream, which is being
          written to a single file. This occurs on each client (physical machine
          or virtual server). But the primary goal of centralized log management
          is to collect logs from many different sources into a single, central
          (offsite) location. To accomplish that goal, we need to ship the logs
        </p>

        <p>
          Each system running a Crato client forwards the stream of captured log
          messges via <code>tcp</code> to the offsite core of Crato's
          infrastructure. To mitigate message loss resulting from a network
          partition or problems with the central server, each Crato client
          implements a disk-assisted memory queue in front of the forwarding
          action.
        </p>

        <p>
          WHY DEFAULT TO TCP?: at our expected volumes, the disk-assisted memory
          queue (a linked list capable of holding 10000 individual log messages,
          with 1Gb disk space available if needed) should mitigate the need to
          fire and forget to keep up with traffic. Thus, Crato opts for TCP and
          its guarantees of delivery and packet order over UDP's potential speed
          gains and absence of guarantees.
        </p>

        <h4 id="622ingestion">
          6.2.2 Ingestion
        </h4>

        <p>
          As it receives new log events from the dispersed clients, the central
          log server appends each message to its own log file. At this point,
          the central collection server's <code>syslog</code> file contains all
          captured messages from the clients along with those from the
          collection server itself. The full text is available for CLI query
          using familiar text processing tools like <code>grep</code>,
          <code>tail</code>, <code>sort</code>, and <code>awk</code>.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_central_server.png"
            alt="Logs from across system read from single place."
          />

          <p>
            Central point for insight into entire system: all messages available
            to familiar unix tools.
          </p>
        </div>

        <p>
          Central log servers also forward logs into Crato’s pub-sub system,
          where they are made available to both the archiving server and to the
          database.
        </p>

        <h3 id="63messagestreaming">
          6.3 Message Streaming
        </h3>

        <h4 id="631weneedtostorelogsbuthow">
          6.3.1 We Need to Store Logs...but How?
        </h4>

        <p>
          It is common practice for log management systems to persist log data
          not only to save it from automatic log rotation jobs, but also to
          facilitate metrics and other system analysis. Crato follows this
          practice as well, with logs being saved for archiving and also made
          available for querying.
        </p>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/generic_dbs.png"
            alt="Central server sending logs to two datastores"
          />
        </div>

        <p>
          But then couldn't Crato's central server send logs directly to the
          persistence layer? Yes, in fact it's earliest iterations did exactly
          that, but this approach is sub-optimal for a number of related
          reasons.
        </p>

        <p>
          Logs are produced at different rates than they are consumed.
          Furthermore, the same log data will be used by multiple processes.
          Each use can be expected to have different needs, to operate at a
          different rate, and to have potentially different bottlenecks or
          failure rates. As a result, sending a copy of each log message to each
          consumer as they come in is likely to lead to dropped messages with no
          good way of recovery. Crato needed a buffering system to capture logs
          and allow each consumer to draw data at its own rate.
        </p>

        <p>
          One way to address this problem would be to use a queue. Log messages
          are sent into a queue, and a consumer shifts the next message off as
          needed. This may would work well for an individual consumer, but Crato
          needs to handle multiple consumers, and as each consumer grabs the
          next available message from the queue, all the others miss out. The
          message is no longer available. What Crato would need would be a queue
          in front of each potential consumer. To implement that, the central
          server would need to send a copy of each log message to n queues in
          front of n consumers.
        </p>

        <p>
          Another potential solution would be to use a publish-subscribe
          messaging system. The central server would publish each new log
          message to the platform, and all subscribers would receive the log as
          it is available. While this model allows the central server to forward
          its stream to a single destination, it does not account for the
          different rates of consumption.
        </p>

        <h4 id="632decouplingloggenerationusewithkafka">
          6.3.2 Decoupling Log Generation &amp; Use with Kafka
        </h4>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_kafka.png"
            alt="Architecture with Kafka highlighted"
          />

          <p></p>
        </div>

        <p>
          What Crato needs is a way to provide the same stream of log messages
          to multiple consumers, to allow producers and consumers to work at
          their own paces, and for each to always be able to resume its work at
          the exact point it left off. It needs the advantages of a queue and a
          pub-sub system folded into a single platform. Apache Kafka provides
          exactly that.
        </p>

        <blockquote>
          <p>
            The consumer group concept in Kafka generalizes these two concepts.
            As with a queue the consumer group allows you to divide up
            processing over a collection of processes (the members of the
            consumer group). As with publish-subscribe, Kafka allows you to
            broadcast messages to multiple consumer groups.

            <a href="https://kafka.apache.org/documentation.html#kafka_mq">
              Kafka as a Messaging System
            </a>
          </p>
        </blockquote>

        <p>
          By decoupling log creation or ingestion from log use, Kafka solves the
          problem noted above, namely Crato's need for exactly once delivery of
          log messages to multiple users operating at different paces. At the
          same time, it also allows Crato to be more extensible in the future.
          Any additional uses for the same data can be added as consumers.
        </p>

        <p>
          But if Kafka is so effective at managing this decoupling, wouldn't
          Crato benefit from skipping the central server model and simply have
          each client produce directly to remote Kafka brokers instead? There
          are advantages to this approach. It would reduce complexity in Crato's
          core infrastructure, and there is no doubt that Kafka could handle the
          load from multiple distributed clients as well as it does from a
          single central log server.
        </p>

        <p>
          That said, this design would increase the complexity on the client
          side by introducing additional configuration requirements and
          dependencies. If we want to use the <code>syslog</code> protocol on
          each client (see the justification above), then an
          <code>rsyslog</code> central server poses fewer problems for client’s
          installation. Also, removing the central server would eliminate one of
          Crato's UI advantages, because without a central collection point it
          would lose the ability to provide system-wide observability using the
          familiar *nix text processing tools. One of the design goals is 'easy
          to use'. Asking users to query Kafka for observability pushes hard
          against that goal.
        </p>

        <h4 id="633producingtokafka">
          6.3.3 Producing to Kafka
        </h4>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/omkafka.png"
            alt="omkafka producing to Kafka brokers"
          />
        </div>

        <p>
          With Kafka in place, Crato's central collection server sends plain
          text and JSON formatted log messages to Kafka brokers via
          <code>rsyslog</code>'s <code>omkafka</code> module. Early tests of
          Crato with a single Kafka broker revealed that the broker would not
          always be available to receive messages. To reduce the potential for
          log loss, Crato is now built with a three broker Kafka cluster, thus
          preserving service even if two brokers fail. Additionally, it employs
          on its central server a disk-assisted memory queue to mitigate
          partitions between server and Kafka cluster, and to provide an
          emergency store of unsent messages if <code>rsyslogd</code> shuts
          down, or in the event that all Kafka brokers are down.
        </p>

        <h4 id="634consumingfromkafka">
          6.3.4 Consuming from Kafka
        </h4>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_consumers2.png"
            alt="consuming from Kafka brokers"
          />
        </div>

        <p>
          Although the logs are buffered, and Kafka provides delivery guarantees
          for Crato's multiple consumers, the actual consumption is not
          automatic. That is, Kafka will hold the messages for their specified
          time-to-live (pending available disk space), but the consumers must
          make the requests and manage flow to suit their endpoint's needs.
        </p>

        <p>
          As a result of this pull model, Crato is built with two consumer
          groups, one for batching and one for streaming. The batching consumers
          save logs to a json file until it reaches a configurable size, and
          then send the file to Amazon S3 Glacier. The streaming consumers
          filter incoming log messages and serve them to their appropriate
          measurements in InfluxDB.
        </p>

        <h3 id="64storage">
          6.4 Storage
        </h3>

        <div class="img-wrapper">
          <img
            src="images/diagrams/crato/crato_storage.png"
            alt="two kinds of storage needs"
          />
        </div>

        <h4 id="641dualstorageneeds">
          6.4.1 Dual Storage Needs
        </h4>

        <p>
          In order to provide the full advantages associated with centralized
          logging, a platform would need to store logs and to make them
          available for queries. This could be accomplished with a single data
          store, but Crato distinguishes between two very different needs here.
          First, (as discussed above) there is value in archiving logs, i.e.
          setting them aside in files for long-term preservation. Second,
          storing individual log messages for metrics based queries provides
          unique insight into a system's performance. Crato's storage is
          designed with the idea that these different needs should be addressed
          separately. As a result, Crato uses two forms of storage.
        </p>

        <h5>Archives:</h5>
        <ul>
          <li>log <i>files</i> saved for as long as desired</li>
          <li>delayed access to logs is acceptable</li>
          <li>stored offsite as a backup</li>
        </ul>

        <h5>Queryable storage:</h5>
        <ul>
          <li>log <i>messages</i> saved for limited time</li>
          <li>immediate access to query is required</li>
          <li>location is not essential</li>
        </ul>

        <h4 id="642archivesamazons3glacier">
          6.4.2 Archives: Amazon S3 Glacier
        </h4>

        <p>
          Crato uses Amazon S3 Glacier for archival log storage. Users enter
          their AWS credentials and a destination S3 bucket name when deploying
          Crato core, and Crato handles the rest. AWS settings and configuration
          are handled directly through Amazon.
        </p>

        <p>
          Using S3 for log file archiving gives Crato's users offsite storage
          with 99.99..% durability and automatic lifecycle management
          (configured via AWS console) at a competitive price. Furthermore,
          because Crato archives in JSON format, the files themselves are
          queryable from AWS console. As a result users may have an easier time
          determining which files they wish to extract.
        </p>

        <h4 id="643queryablestorageneeds">
          6.4.3 Queryable Storage Needs
        </h4>

        <p>
          Emphasizing metrics-based queries, Crato should be able to track
          entities (e.g. servers, applications, processes) through thier logs as
          they change across time. Seen in this light, logs can be understood as
          containing time series data, and as ideal candidates for time series
          databases. Consider the following comparisons.
        </p>

        <p>
          <b>Time Series Data</b>: sequences of data points from individual
          sources over time
        </p>

        <ul>
          <li>Time is one axis when TS data is plotted on a graph</li>
          <li>Relationships between entities (tables) not important</li>
        </ul>

        <p>
          <b>Time Series Databases</b>: databases built for collecting, storing,
          retrieving &amp; processing TS data
        </p>

        <ul>
          <li>
            Retention policy / aggregation of fine-grained data as it ages
          </li>
          <li>Query &amp; summarization tools</li>
          <li>Integration with metrics visualization tools</li>
        </ul>

        <p>
          Although other forms of database can be used for time series data,
          they often lack the tooling to make working with time series easy .
          For example, running a continuous query over a short retention period
          measurement allows live monitoring of a server's response rate. Also,
          time series databases tend to be optimized for fast indexing by time.
          While other database types tend to slow down due to indexing as the
          data set grows, time series databases tend to keep a consistently high
          ingestion rate.
        </p>

        <h4 id="644queryablestorageinfluxdb">
          6.4.4 Queryable Storage: InfluxDB
        </h4>

        <p>
          Crato deploys an instance of InfluxDB for queryable storage. Since
          most time series databases have similar optimizations, and they tend
          to be immersed in an ecosystem that allows easy integration with
          visualization tools, the decisive factor in using Influx is the fact
          that it is schema-free. As a result, when a Crato consumer sends logs
          to InfluxDB it automatically creates a new measurement (table) for
          each new log source. That means that Crato's users can benefit
          immediately, and will also be able to add new clients without having
          to declare new schema.
        </p>

        <h2 id="challenges">7 Challenges</h2>

        <h3 id="71collectlogs">
          7.1 Collect Logs
        </h3>

        <p>
          How to efficiently and reliably collect logs without imposing on the
          developers' application code?
        </p>

        <p>Common collection options not chosen by Crato:</p>
        <ul>
          <li>
            Install active collection agent on users' servers
            <ul>
              <li>Pros: Crato would have control</li>
              <li>
                Cons: invasive; violates goal of being lighweight; risks of
                adding code into an untested environment
              </li>
            </ul>
          </li>
          <li>
            Integrate with an application logging library
            <ul>
              <li>Pros: many libraries already familiar to developers</li>
              <li>
                Cons: narrows the scope of users to those using a particular
                library or framework; likely to not incorportate web server logs
                or system logs
              </li>
            </ul>
          </li>
        </ul>

        <p>
          Crato's approach: Adapt <code>rsyslogd</code>, an already in place
          system-level logging daemon to collect from applications as well.
          Create customized configurations that supplement, not override,
          existing <code>rsyslog</code> configurations.
        </p>

        <h3 id="72producingtokafka">
          7.2 Producing to Kafka
        </h3>

        <h3 id="73docker">
          7.3 Docker
        </h3>

        <p>
          crashed broker causes problems through out pipeline. Must identfy and
          orchestrate restart sequence.
        </p>

        <h2 id="future-plans">8 Future Plans</h2>

        <p>Additional visualization integration</p>

        <p>Security</p>

        <ul>
          <li>option for LAN -client-server communication with TLS</li>
        </ul>

        <p>Compression option for large messages</p>

        <section id="footnotes">
          <h2 id="references">9 References</h2>

          <h5>9.1 Footnotes</h5>

          <ol>
            <li id="footnote-1">
              <a href="https://en.wikipedia.org/wiki/Log_file" target="_blank"
                >https://en.wikipedia.org/wiki/Log_file</a
              >
            </li>
            <li id="footnote-2">
              <a
                href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying"
                target="_blank"
                >https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying
              </a>
            </li>
            <li id="footnote-3">
              <a href="https://12factor.net/" target="_blank"
                >https://12factor.net/
              </a>
            </li>

            <li id="footnote-4">
              <a
                href="https://docs.confluent.io/current/streams/concepts.html"
                target="_blank"
                >https://docs.confluent.io/current/streams/concepts.html
              </a>
            </li>
            <li id="footnote-5">
              <a
                href="https://cruft.io/posts/centralised-logging-with-rsyslog/"
                target="_blank"
                >https://cruft.io/posts/centralised-logging-with-rsyslog/
              </a>
            </li>
          </ol>
          <!--

          <h5>9.2 Resources</h5>

          <ol>

            <li><a

                href="https://www.manning.com/books/serverless-architectures-on-aws"

                target="_blank">Serverless Architecture on AWS</a>

            </li>

          </ul>

-->
        </section>
      </section>
    </main>

    <section id="our-team">
      <h1>Our Team</h1>

      <p>
        We are looking for opportunities. If you liked what you saw and want to
        talk more, please reach out!
      </p>

      <ul>
        <li class="individual">
          <img src="https://github.com/4g3m.png?" alt="Faazil Shaikh" />

          <h3>Faazil Shaikh</h3>

          <p>San Francisco, CA</p>

          <ul class="social-icons">
            <li>
              <a href="mailto:faazil.shaikh@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>

            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>

            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>

        <li class="individual">
          <img
            src="https://github.com/jkurthoconnor.png"
            alt="Kurth O'Connor"
          />

          <h3>Kurth O'Connor</h3>

          <p>New York, NY</p>

          <ul class="social-icons">
            <li>
              <a href="mailto:jkurthoconnor@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>

            <li>
              <a href="http://kurthoconnor.com" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>

            <li>
              <a
                href="https://www.linkedin.com/in/kurth-o-connor-01986a169"
                target="_blank"
              >
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>

        <li class="individual">
          <img src="https://github.com/alex-solo.png" alt="Alex Soloviev" />

          <h3>Alex Soloviev</h3>

          <p>Toronto, Canada</p>

          <ul class="social-icons">
            <li>
              <a href="mailto:alex.soloviev@gmail.com" target="_blank">
                <img src="images/icons/email_icon.png" alt="email" />
              </a>
            </li>

            <li>
              <a href="https://" target="_blank">
                <img src="images/icons/website_icon.png" alt="website" />
              </a>
            </li>

            <li>
              <a href="https://www.linkedin.com/" target="_blank">
                <img src="images/icons/linked_in_icon.png" alt="linkedin" />
              </a>
            </li>
          </ul>
        </li>
      </ul>
    </section>
  </body>
</html>
